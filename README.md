# MLP

- MultiLayerNet.py

-> Implementation code for MLP architecture

- Training.py

-> Training code

- nn.functions

-> Implementation code of xavier, he weight initialization, activation functions, loss functions and affine layer

- nn.optim

-> Implementation code of optimizers

List of optimizers

1. SGD

2. Momentum

3. Adagrad

4. RMSprop

5. Adadelta

6. Adam

- nn.utils

-> Implementation code of early stopping, dataloader and gradient clipping


- Training results
1. Accs by number of layer

![original](https://user-images.githubusercontent.com/77143331/235698613-fbe8965f-0ffc-43c6-b801-2ec7ecc05a1a.png)

2-1. Accs by learning rate

![original-2](https://user-images.githubusercontent.com/77143331/235698773-7bd2dadd-f8e4-4c9e-a989-7e53d8e03f77.png)

2-2. Accs by learning rate

![original-3](https://user-images.githubusercontent.com/77143331/235698859-327370af-e852-4ff8-b30c-e6b9b12a3eab.png)
