# MLP

- MultiLayerNet.py

-> Implementation code for MLP architecture

- Training.py

-> Training code

- nn.functions

-> Implementation code of xavier, he weight initialization, activation functions, loss functions and affine layer

- nn.optim

-> Implementation code of optimizers

List of optimizers

1. SGD

2. Momentum

3. Adagrad

4. RMSprop

5. Adadelta

6. Adam

- nn.utils

-> Implementation code of early stopping, dataloader and gradient clipping
